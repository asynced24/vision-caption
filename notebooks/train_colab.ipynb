{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Vision-Caption Projector Training\n",
        "\n",
        "This notebook trains the projector component of the vision-caption model on COCO Captions dataset.\n",
        "\n",
        "**What this trains:** Only the MLP projector that maps vision features to language embeddings.\n",
        "\n",
        "**What stays frozen:** SigLIP vision encoder and Qwen2-1.5B language decoder.\n",
        "\n",
        "**Dataset:** COCO train2017 (~118k images, ~590k captions)\n",
        "\n",
        "**Training time:** ~2-3 hours on T4 GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone repository\n",
        "!git clone https://github.com/asynced24/vision-caption.git\n",
        "%cd vision-caption"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "%pip install -e . -q\n",
        "%pip install pycocotools -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download COCO Dataset\n",
        "\n",
        "Full COCO train2017 dataset: 118k images (~18GB) + annotations (~1GB).\n",
        "\n",
        "**Direct links:**\n",
        "- Images: http://images.cocodataset.org/zips/train2017.zip\n",
        "- Annotations: http://images.cocodataset.org/annotations/annotations_trainval2017.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Create data directory\n",
        "data_dir = Path(\"coco_data\")\n",
        "data_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Download train2017 images (~18GB)\n",
        "if not (data_dir / \"train2017\").exists():\n",
        "    print(\"Downloading COCO train2017 images...\")\n",
        "    !wget http://images.cocodataset.org/zips/train2017.zip -P coco_data\n",
        "    !unzip -q coco_data/train2017.zip -d coco_data\n",
        "    !rm coco_data/train2017.zip\n",
        "    print(\"✓ Images downloaded\")\n",
        "else:\n",
        "    print(\"✓ Images already downloaded\")\n",
        "\n",
        "# Download annotations (~1GB)\n",
        "if not (data_dir / \"annotations\").exists():\n",
        "    print(\"Downloading COCO annotations...\")\n",
        "    !wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip -P coco_data\n",
        "    !unzip -q coco_data/annotations_trainval2017.zip -d coco_data\n",
        "    !rm coco_data/annotations_trainval2017.zip\n",
        "    print(\"✓ Annotations downloaded\")\n",
        "else:\n",
        "    print(\"✓ Annotations already downloaded\")\n",
        "\n",
        "print(f\"\\nDataset ready!\")\n",
        "print(f\"Images: {data_dir / 'train2017'}\")\n",
        "print(f\"Captions: {data_dir / 'annotations' / 'captions_train2017.json'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Configuration\n",
        "\n",
        "Adjust parameters below. For quick testing, set `MAX_SAMPLES = 10000`. For full training, set `MAX_SAMPLES = None`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "IMAGES_DIR = \"coco_data/train2017\"\n",
        "ANNOTATIONS_FILE = \"coco_data/annotations/captions_train2017.json\"\n",
        "OUTPUT_DIR = \"checkpoints\"\n",
        "EPOCHS = 3\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 1e-3\n",
        "MAX_SAMPLES = None  # None = full dataset (~118k images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Start Training\n",
        "\n",
        "This will take ~2-3 hours on T4 GPU for the full dataset. Loss should decrease from ~0.5 to ~0.05."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cmd = f\"python train.py --images-dir {IMAGES_DIR} --annotations-file {ANNOTATIONS_FILE} --output-dir {OUTPUT_DIR} --epochs {EPOCHS} --batch-size {BATCH_SIZE} --lr {LEARNING_RATE}\"\n",
        "if MAX_SAMPLES:\n",
        "    cmd += f\" --max-samples {MAX_SAMPLES}\"\n",
        "    \n",
        "!{cmd}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Trained Model\n",
        "\n",
        "Generate a caption with the newly trained projector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from vision_caption import ModelConfig, load_model\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "# Load model with trained projector\n",
        "config = ModelConfig()\n",
        "config.projector_path = \"checkpoints/projector_final.pt\"\n",
        "model = load_model(config)\n",
        "\n",
        "# Test with an example image\n",
        "url = \"https://images.unsplash.com/photo-1518791841217-8f162f1e1131\"\n",
        "response = requests.get(url)\n",
        "image = Image.open(BytesIO(response.content))\n",
        "\n",
        "# Generate caption\n",
        "caption = model.generate(image)\n",
        "print(f\"\\nGenerated caption: {caption}\")\n",
        "\n",
        "display(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download Trained Weights\n",
        "\n",
        "Save the trained projector to use locally or share."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Download the final trained projector\n",
        "files.download('checkpoints/projector_final.pt')\n",
        "\n",
        "print(\"\\n✓ Downloaded projector_final.pt\")\n",
        "print(\"\\nTo use it:\")\n",
        "print(\"1. Place it in your project's 'checkpoints/' directory\")\n",
        "print(\"2. Set config.projector_path = 'checkpoints/projector_final.pt'\")\n",
        "print(\"3. Run: model = load_model(config)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Launch Gradio Demo (Optional)\n",
        "\n",
        "Test the trained model interactively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Update colab_app to use trained weights\n",
        "import fileinput\n",
        "\n",
        "for line in fileinput.input('colab_app.py', inplace=True):\n",
        "    if 'config = ModelConfig()' in line:\n",
        "        print(line, end='')\n",
        "        print('    config.projector_path = \"checkpoints/projector_final.pt\"')\n",
        "    else:\n",
        "        print(line, end='')\n",
        "\n",
        "# Launch demo with trained model\n",
        "!python colab_app.py"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
